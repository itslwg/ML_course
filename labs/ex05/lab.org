* Activate the environment and import packages

  #+BEGIN_SRC elisp :session
(pyvenv-activate "~/courses/Machine Learning")
  #+END_SRC

  #+RESULTS:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
# Useful starting lines
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
%load_ext autoreload
%autoreload 2
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[1]:
  :end:
  
* 1. Classification using Linear Regression

  Import lab specific methods
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
from template.helpers import (
    load_data,
    build_model_data,
    standardize,
    sample_data
)
from template.plots import visualization
from template.least_squares import least_squares
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[88]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
# load data.
height, weight, gender = load_data()

# build sampled x and y.
seed = 1
y = np.expand_dims(gender, axis=1)
X = np.c_[height.reshape(-1), weight.reshape(-1)]
y, X = sample_data(y, X, seed, size_samples=200)
x, mean_x, std_x = standardize(X)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[333]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def least_square_classification_demo(y, x):
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    rmse, w = least_squares(y, tx)

    visualization(y, x, mean_x, std_x, w, "classification_by_least_square")
    
least_square_classification_demo(y, x)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[334]:
  [[file:./obipy-resources/zRIeTK.png]]
  :end:

* 1. Logistic Regression
** 1.1 Logistic regression demo
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def sigmoid(t):
    """apply the sigmoid function on t."""
    return np.exp(t) / (1 + np.exp(t))
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[115]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def calculate_loss(y, tx, w):
    """compute the loss: negative log likelihood."""
    h = sigmoid(tx@w)
    loss = (1/len(y)) * (-y.T @ np.log(h) - (1 - y).T @ np.log(1 - h))
    
    return loss
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[431]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def calculate_gradient(y, tx, w):
    """compute the gradient of loss."""
    g = (1 / len(y)) * tx.T @ (sigmoid(tx @ w) - y)
    return  g
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[443]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def learning_by_gradient_descent(y, tx, w, gamma):
    """
    Do one step of gradient descent using logistic regression.
    Return the loss and the updated w.
    """
    loss = calculate_loss(y, tx, w)
    g = calculate_gradient(y, tx, w)
    w = w - gamma * g
    
    return loss, w
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[444]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
from template.helpers import de_standardize

def logistic_regression_gradient_descent_demo(y, x):
    # init parameters
    max_iter = 10000
    threshold = 1e-8
    gamma = 0.01
    losses = []

    # build tx
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = np.zeros((tx.shape[1], 1))
    # start the logistic regression
    for iter in range(max_iter):
        # get loss and update w.
        loss, w = learning_by_gradient_descent(y, tx, w, gamma)
        print(loss)
        # log info
        if iter % 100 == 0:
            print("Current iteration={i}, loss={l}".format(i=iter, l=loss))
        # converge criterion
        losses.append(loss)
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break
    # visualization
    visualization(y, x, mean_x, std_x, w, "classification_by_logistic_regression_gradient_descent", True)
    print("loss={l}".format(l=calculate_loss(y, tx, w)))

logistic_regression_gradient_descent_demo(y, x)
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[446]:
  [[file:./obipy-resources/aHQqgx.png]]
  :end:
  
** 1.2 Newton's method for logistic regression

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
tx.T @ np.diag((sigmoid(tx @ w) * sigmoid(tx @ w))) @ tx
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[312]:
   #+BEGIN_EXAMPLE
     array([[ 78.08602087, -19.3661688 , -22.48748182],
     [-19.3661688 ,  77.32413335,  71.64621139],
     [-22.48748182,  71.64621139,  79.0291408 ]])
   #+END_EXAMPLE
   :end:
   

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def calculate_hessian(y, tx, w):
    """return the Hessian of the loss function."""
    Snn = sigmoid(tx @ w) * (1 - sigmoid(tx @ w))
    S = np.diag(Snn.ravel())
    H = tx.T @ S @ tx
    
    return H
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[447]:
   :end:

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def logistic_regression(y, tx, w):
    """return the loss, gradient, and Hessian."""
    return (
        calculate_loss(y, tx, w),
        calculate_gradient(y, tx, w),
        calculate_hessian(y, tx, w)
    )
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[448]:
   :end:

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def learning_by_newton_method(y, tx, w, gamma):
    """
    Do one step on Newton's method.
    return the loss and updated w.
    """
    loss, g, H = logistic_regression(y, tx, w)
    w = w - gamma * np.linalg.inv(H) @ w

    return loss, w
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[449]:
   :end:
   
   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def logistic_regression_newton_method_demo(y, x):
    # init parameters
    max_iter = 100
    threshold = 1e-8
    lambda_ = 0.1
    gamma = 1.
    losses = []

    # build tx
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = np.zeros((tx.shape[1], 1))

    # start the logistic regression
    for iter in range(max_iter):
        # get loss and update w.
        loss, w = learning_by_newton_method(y, tx, w, gamma)
        # log info
        if iter % 1 == 0:
            print("Current iteration={i}, the loss={l}".format(i=iter, l=loss))
        # converge criterion
        losses.append(loss)
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break
    # visualization
    visualization(y, x, mean_x, std_x, w, "classification_by_logistic_regression_newton_method",True)
    print("loss={l}".format(l=calculate_loss(y, tx, w)))

logistic_regression_newton_method_demo(y, x)
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[450]:
   [[file:./obipy-resources/cLWxtE.png]]
   :end:
   
** 1.3 Penalized Logistic Regression

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
tx = np.c_[np.ones((y.shape[0], 1)), x]
w = np.ones((tx.shape[1], 1))
calculate_penalized_loss(y, tx, w, 0.000000001)
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[451]:
   : array([[1.87449916]])
   :end:
   
   
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def calculate_penalized_loss(y, tx, w, lambda_):
    """compute the loss: negative log likelihood."""
    loss = calculate_loss(y, tx, w) + lambda_ * np.linalg.norm(w, 2) ** 2
    
    return loss
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[452]:
  :end:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def calculate_penalized_gradient(y, tx, w, lambda_):
    """compute the gradient of loss."""
    g = calculate_gradient(y, tx, w) + 2 * lambda_ * w
    return  g
  #+END_SRC

  #+RESULTS:
  :results:
  # Out[453]:
  :end:

  
   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def penalized_logistic_regression(y, tx, w, lambda_):
    """return the loss, gradient"""
    return (
        calculate_penalized_loss(y, tx, w, lambda_),
        calculate_penalized_gradient(y, tx, w, lambda_)
    )
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[454]:
   :end:

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):
    """
    Do one step of gradient descent, using the penalized logistic regression.
    Return the loss and updated w.
    """
    loss, g = penalized_logistic_regression(y, tx, w, lambda_)
    w = w - gamma * g
    return loss, w, g
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[465]:
   :end:

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def logistic_regression_penalized_gradient_descent_demo(y, x):
    # init parameters
    max_iter = 10000
    gamma = 0.01
    lambda_ = 0.4
    threshold = 1e-8
    losses = []

    # build tx
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = np.zeros((tx.shape[1], 1))

    # start the logistic regression
    for iter in range(max_iter):
        # get loss and update w.
        loss, w, g = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)
        # log info
        if iter % 100 == 0:
            print("Current iteration={i}, loss={l}, normw={g}".format(i=iter, l=loss, g=np.linalg.norm(g)))
        # converge criterion
        losses.append(loss)
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break
    # visualization
    visualization(y, x, mean_x, std_x, w, "classification_by_logistic_regression_penalized_gradient_descent",True)
    print("loss={l}".format(l=calculate_penalized_loss(y, tx, w, lambda_)))
    
logistic_regression_penalized_gradient_descent_demo(y, x)
   #+END_SRC

   #+RESULTS:
   :results:
   # Out[470]:
   [[file:./obipy-resources/RbjS6C.png]]
   :end:
   
   
   
