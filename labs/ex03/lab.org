* Activate the environment and import packages 

  #+BEGIN_SRC elisp :session
(pyvenv-activate "~/courses/Machine Learning")
  #+END_SRC

  #+RESULTS:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
# Useful starting lines
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
import os
%load_ext autoreload
%autoreload 2
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[2]:
  :END:

* Load the data

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
import datetime
from template.helpers import *

height, weight, gender = load_data_from_ex02(sub_sample=False)
x, mean_x, std_x = standardize(height)
y, tx = build_model_data(x, weight)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[3]:
  :END:
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
y.shape, tx.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  : ((10000,), (10000, 2))
  :END:
  
* 1. Least Squares and Linear Basis Functions models
** Least Squares
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def least_squares(y, tx):
    """calculate the least squares solution."""
    w = np.dot(np.linalg.inv(np.dot(tx.T, tx)), np.dot(tx.T, y))
    X = y[np.newaxis].T - np.dot(tx, w[np.newaxis].T)
    rmse = np.sqrt(1/tx.shape[0] * np.dot(X.T, X))

    return rmse, w
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[92]:
  :END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
rmse, w = least_squares(y, tx)
rmse
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[93]:
  : array([[0.47187608]])
  :END:
 
  Comparison with the gradient descent method...

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def compute_gradient(y, tx, w):
    """Compute the gradient."""
    e = y[np.newaxis].T - np.dot(tx, w[np.newaxis].T)
    return - ((1 / tx.shape[0]) * np.dot(tx.T, e)).T
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[94]:
  :END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def compute_loss(y, tx, w):
    """Calculate the loss.

    You can calculate the loss using mse or mae.
    """
    e = y[np.newaxis].transpose() - tx.dot(w[np.newaxis].T)
    mse = (1 / (2*tx.shape[0])) * np.sum(np.square(e))
    
    return mse

def gradient_descent(y, tx, initial_w, max_iters, gamma):
    """Gradient descent algorithm."""
    # Define parameters to store w and loss
    ws = [initial_w]
    losses = []
    w = initial_w
    for n_iter in range(max_iters):
        # Compute gradient and loss
        g = compute_gradient(y, tx, w)
        loss = compute_loss(y, tx, w)
        # Update the weights
        w = w - gamma * g
        w = w.ravel()
        # store w and loss
        ws.append(w)
        losses.append(loss)
        print("Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}".format(
              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))

    return losses, ws
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[95]:
  :END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
losses, ws = gradient_descent(y, tx, np.array([0, 0]), 50, 0.5)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[9]:
  :END:

** Least squares with a linear basis function model

   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
# load dataset
x, y = load_data()
"shape of x {}".format(x.shape)
"shape of y {}".format(y.shape)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[10]:
   : 'shape of y (50,)'
   :END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
y, tx = build_model_data(x, y)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[11]:
  :END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
degree = 5
augmentation = np.zeros((tx.shape[0], degree))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[12]:
  :END:
  
   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def build_poly(x, degree):
    """polynomial basis functions for input data x, for j=0 up to j=degree."""
    # Do augmentation
    X_augmented = np.hstack([np.ones((x.shape[0], 1)), x[np.newaxis].T])

    if degree != 1:
        for j in np.arange(2, degree + 1):
            X_augmented = np.hstack([X_augmented, np.power(x, j)[np.newaxis].T])

    return X_augmented
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[13]:
   :END:
   
   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
from template.plots import *

def compute_loss(y, tx, w):
    """Compue RMSE loss in matrix form."""
    e = y[np.newaxis].T - np.dot(tx, w[np.newaxis].T)
    rmse = np.sqrt(1/tx.shape[0] * np.dot(e.T, e))
    
    return rmse.ravel()

def least_squares(y, tx):
    """calculate the least squares solution."""
    w = np.dot(np.linalg.inv(np.dot(tx.T, tx)), np.dot(tx.T, y))
    rmse = compute_loss(y, tx, w)

    return rmse, w

def polynomial_regression(x):
    """Constructing the polynomial basis function expansion of the data,
       and then running least squares regression."""
    # define parameters
    degrees = [1, 3, 7, 12]
    
    # define the structure of the figure
    num_row = 2
    num_col = 2
    f, axs = plt.subplots(num_row, num_col)

    for ind, degree in enumerate(degrees):
        # Do augmentation
        X_augmented = build_poly(x, degree)
        # Calculate rmse
        rmse, weights = least_squares(y, X_augmented)
        print("Processing {i}th experiment, degree={d}, rmse={loss}".format(
              i=ind + 1, d=degree, loss=rmse))
        # plot fit
        plot_fitted_curve(
            y, x, weights, degree, axs[ind // num_col][ind % num_col])
    plt.tight_layout()
    plt.savefig("visualize_polynomial_regression")
    plt.show()
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[129]:
   :END:
   
   #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
polynomial_regression(x)
   #+END_SRC

   #+RESULTS:
   :RESULTS:
   # Out[130]:
   [[file:./obipy-resources/ZTfLjH.png]]
   :END:
   
* 2. Evaluating Model Prediction Performance

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
ratio = 0.6
n_samples = np.int(np.floor(x.shape[0] * ratio))
np.random.randint(0, x.shape[0], n_samples)
np.in1d(np.arange(x.shape[0]), np.random.randint(0, x.shape[0], n_samples), invert=True)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[131]:
  #+BEGIN_EXAMPLE
    array([ True, False, False, False, False, False, False,  True,  True,
    False,  True,  True, False, False, False,  True,  True,  True,
    False,  True,  True, False, False,  True, False, False,  True,
    True,  True, False,  True,  True,  True, False,  True,  True,
    False, False,  True,  True, False, False, False,  True,  True,
    True,  True, False, False, False])
  #+END_EXAMPLE
  :END:
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
a = np.array([1,2,3,4,5])
a[:3]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[252]:
  : array([1, 2, 3])
  :END:
  

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def split_data(x, y, ratio, seed=1):
    """
    split the dataset based on the split ratio. If ratio is 0.8 
    you will have 80% of your data set dedicated to training 
    and the rest dedicated to testing
    """
    # set seed
    np.random.seed(seed)

    # setup indices
    n_train_samples = np.int(np.floor(x.shape[0] * ratio))
    train_indices = np.random.randint(0, x.shape[0], n_train_samples)
    test_indices = np.in1d(np.arange(x.shape[0]), train_indices, invert=True)
    # Split the data
    X_train, y_train = x[train_indices], y[train_indices]
    X_test, y_test = x[test_indices], y[test_indices]

    return X_train, y_train, X_test, y_test
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[286]:
  :END:
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
xt, yt, xtt, ytt = split_data(x, y, 0.8)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[271]:
  :END:
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
xt
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[272]:
  #+BEGIN_EXAMPLE
    array([ 1.14338223, -0.79690277, -1.28197402,  1.69774938, -0.4504233 ,
    0.24253563, -1.35126991,  1.21267812, -1.55915759,  1.62845348,
    0.93549455, -1.28197402,  1.35126991,  0.03464795,  0.38112741,
    -1.00479045,  0.72760688,  1.14338223, -1.4898617 ,  0.72760688,
    0.24253563, -0.03464795,  0.58901509, -1.07408634, -0.86619866,
    0.10394384, -1.69774938, -1.69774938,  0.86619866,  1.07408634,
    -0.5197192 , -0.93549455, -1.14338223, -0.38112741,  0.86619866,
    -0.4504233 ,  1.35126991, -1.14338223,  0.93549455, -1.55915759])
  #+END_EXAMPLE
  :END:
  

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def train_test_split_demo(x, y, degree, ratio, seed):
    """polynomial regression with different split ratios and different degrees."""
    # Split the data
    X_train, y_train, X_test, y_test = split_data(x, y, ratio, seed)
    # Form linear basis functions 
    Xt_train = build_poly(X_train, degree)
    Xt_test = build_poly(X_test, degree)
    # Conduct least-squares
    _, w = least_squares(y_train, Xt_train)
    # Compute performance measures
    rmse_tr, rmse_te = compute_loss(y_train, Xt_train, w)[0], compute_loss(y_test, Xt_test, w)[0]
    
    print("proportion={p}, degree={d}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}".format(
          p=ratio, d=degree, tr=rmse_tr, te=rmse_te))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[273]:
  :END:
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
seed = 6
degrees = [1, 3, 7, 12]
split_ratios = [0.9, 0.5, 0.1]

for split_ratio in split_ratios:
    for degree in degrees:
        train_test_split_demo(x, y, degree, split_ratio, seed)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[274]:
  :END:
 
* 3. Ridge Regression

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
def compute_ridge_loss(y, tx, w, lambda_):
    """Compue RMSE loss in matrix form."""
    e = y[np.newaxis].T - np.dot(tx, w[np.newaxis].T)
    rmse = np.sqrt(1/(tx.shape[0]) * np.dot(e.T, e) + lambda_ * np.dot(w, w.T))

    return rmse.ravel()


def ridge_regression(y, tx, lambda_):
    """Implement ridge regression."""
    a = (tx.T @ tx) + lambda_ * 2 * tx.shape[0] * np.eye(tx.shape[1])
    b = tx.T @ y
    w = np.linalg.solve(a, b)
    rmse = compute_ridge_loss(y, tx, w, lambda_)

    return rmse, w
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[257]:
  :END:

  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
tx = build_poly(x, 3)
rmse, w = ridge_regression(y, tx, 0)
rmse_ls, w_ls = least_squares(y, tx)
rmse
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[258]:
  : array([0.25858278])
  :END:
  
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async 
def ridge_regression_demo(x, y, degree, ratio, seed):
    """Ridge regression demo."""
    # define parameter
    lambdas = np.logspace(-5, 0, 15)
    # Split the data
    X_train, y_train, X_test, y_test = split_data(x, y, ratio, seed)
    # Form linear basis functions 
    Xt_train = build_poly(X_train, degree)
    Xt_test = build_poly(X_test, degree)
    rmse_tr = []
    rmse_te = []
    for ind, lambda_ in enumerate(lambdas):
        _, w = ridge_regression(y_train, Xt_train, lambda_)
        rmse_tr.append(compute_ridge_loss(y_train, Xt_train, w, lambda_)[0])
        rmse_te.append(compute_ridge_loss(y_test, Xt_test, w, lambda_)[0])
        print("proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}".format(
               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))
        
    # Plot the obtained results
    plot_train_test(rmse_tr, rmse_te, lambdas, degree)

  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[287]:
  :END:
  
  #+BEGIN_SRC ipython :session :exports both :results raw drawer :async t
seed = 56
degree = 7
split_ratio = 0.5
ridge_regression_demo(x, y, degree, split_ratio, seed)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[288]:
  [[file:./obipy-resources/jmiU1B.png]]
  :END:
  
    
  
* Theory questions

  4. A ill-conditioned system is where a small change in the input matrix causes a large change 
     in the solution. A way to check this is to use the *condition number*.
